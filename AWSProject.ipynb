{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5afa79-8309-48d6-9a09-0deae984365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eventVersion                                                0\n",
      "eventTime                                                   0\n",
      "eventSource                                                 0\n",
      "eventName                                                   0\n",
      "awsRegion                                                   0\n",
      "sourceIPAddress                                             0\n",
      "userAgent                                                   0\n",
      "requestParameters                                           0\n",
      "responseElements                                           50\n",
      "requestID                                                   0\n",
      "eventID                                                     0\n",
      "readOnly                                                    0\n",
      "eventType                                                   0\n",
      "managementEvent                                             0\n",
      "recipientAccountId                                          0\n",
      "eventCategory                                               0\n",
      "sessionCredentialFromConsole                                0\n",
      "is_malicious                                                0\n",
      "userIdentity_type                                           0\n",
      "userIdentity_principalId                                    0\n",
      "userIdentity_arn                                            0\n",
      "userIdentity_accountId                                      0\n",
      "userIdentity_accessKeyId                                    0\n",
      "userIdentity_sessionContext_attributes_creationDate         0\n",
      "userIdentity_sessionContext_attributes_mfaAuthenticated     0\n",
      "tlsDetails_tlsVersion                                       0\n",
      "tlsDetails_cipherSuite                                      0\n",
      "tlsDetails_clientProvidedHostHeader                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas import json_normalize\n",
    "import io \n",
    "\n",
    "#Create the with open block so that you can read the raw contents of you json file for aws\"\n",
    "\n",
    "#Use this to silence all future warnings\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "\n",
    "#We want to analyze each line as its own events in order \n",
    "\n",
    "#This can only go through one event, we want to go through multiple events and logs(multiple JSON objects) in order to train our model\n",
    "#with open(\"my_aws.json\",\"r\") as f:\n",
    "    #data = json.load(f)\n",
    "\n",
    "\n",
    "#Step 1: Create a list to store extracted JSON objects\n",
    "#extracted_aws_events = []\n",
    "\n",
    "#Step 2: We must go through each obejct and read the JSON file line by line\n",
    "#This is for NSJSON\n",
    "#We can use a for loop to go through and use the json.loads() function for each line\n",
    "#with open('my_aws.json', 'r') as file:\n",
    "   # for line in file:\n",
    "        #aws_obj = json.loads(line)\n",
    "        #extracted_aws_events.append(aws_obj)\n",
    "\n",
    "#We use this for multiple JSON objects and we are extracting the inner records list for all the multiple JSON objects\n",
    "with open(\"aws_logs_5000.json\", \"r\") as file:\n",
    "    #Now we have to normalize the data to flatten the nested strcuture and seprate the line\n",
    "    #We are grabbing the real CloudTrail events so in this case of 10+ event dicts\n",
    "    df = pd.json_normalize(json.load(file)['Records'], sep=\"_\")\n",
    "\n",
    "\n",
    "#Now we can clean our data - means getting rid of any bad data in our JSON file\n",
    "\n",
    "#This function allows us the fill in any value in our JSON file that has any null values just test out our function\n",
    "df.fillna({'requestParameters': 'value'}, inplace=True)\n",
    "\n",
    "#Using this fucntion to handle any more remaining nulls or inconsistent data\n",
    "null = df.isnull().sum()\n",
    "print(null)\n",
    "\n",
    "#Filtering our time and making it into a datatype object\n",
    "df['eventTime'] = pd.to_datetime(df['eventTime'])\n",
    "\n",
    "#Allows us to  filter our data based on position based selection\n",
    "df.iloc[0:1,0:10]\n",
    "\n",
    "#We want to remove the duplicate that is contained within our dataset \n",
    "# we have  unhashable lists/dictionaries so we need to change this, we do this so we dont get an TypeError\n",
    "#We select on the coloumns where all values are hashable\n",
    "hashable_cols = [\n",
    "    c for c in df.columns\n",
    "    if df[c].apply(lambda v: isinstance(v, (int, float, str, bool, pd.Timestamp, type(None)))).all()\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "#Check to see if we have any duplicates\n",
    "# print(df.duplicated()) \n",
    "\n",
    "#If we do, we can remove them with this fucntion\n",
    "\n",
    "#We want to add hashable_cols since we \n",
    "#We will add our subset so that we have hashabel_cols \n",
    "\n",
    "df.drop_duplicates(subset=hashable_cols, inplace = True)\n",
    "\n",
    "#Now we can do feature engineering - transdoms raw data into informatives features for machine learning models\n",
    "\n",
    "#We want to extract certain data and turn them into features\n",
    "\n",
    "# Extracting Date from our DateTime Object\n",
    "#df['date'] = df['eventTime'].dt.date\n",
    "\n",
    "# Extracting Time from our DateTime Object\n",
    "df['time'] = df['eventTime'].dt.time\n",
    "\n",
    "# Extracting Hour from our DateTime Object\n",
    "df['hour'] = df['eventTime'].dt.hour\n",
    "\n",
    "# Extracting Month from our DateTime Object\n",
    "df['month'] = df['eventTime'].dt.month_name()\n",
    "\n",
    "# Extracting Day from our DateTime Object\n",
    "df['day'] = df['eventTime'].dt.day_name()\n",
    "\n",
    "# Extracting Year from our DateTime Object\n",
    "df['year'] = df['eventTime'].dt.year\n",
    "\n",
    "#Now we want to encode categorical data such as eventName and userIdentity.type\n",
    "#We can do labeling encoding(coverting column into number)  or one hot encoding(convert each catergory value into a new column and assign\n",
    "# a True or False value to it (1(True) / 0(False)\n",
    "\n",
    "#One Time Encoding \n",
    "#You can encode everything once\n",
    "df = pd.get_dummies(df, columns=[\"eventName\",\"sourceIPAddress\", \"userAgent\", \"awsRegion\"])\n",
    "\n",
    "#Turn your data that you want to(e.g. suspicious IP Addresses) into numeric data\n",
    "#For this value we convert the type to boolean so it doesnt allow missing values\n",
    "#Then, we replace any the missing values with False, finally convert the booleans\n",
    "#to integer so we can encode it\n",
    "df['is_malicious'] = (\n",
    "    df['is_malicious']\n",
    "    .astype('bool')\n",
    "    .fillna(False)\n",
    "    .astype('int8')\n",
    ")\n",
    "\n",
    "#Now we want to keep what we want to feed our maching learning model and drop the rest of the columns\n",
    "cols_to_drop = ['eventTime', 'time', 'day'] #We are keeping the columns for hour, year, date, and month\n",
    "\n",
    "#Also drop the null long string\n",
    "cols_to_drop += ['requestParameters', 'responseElements']\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "#Now we can save all of our data to a csv file\n",
    "df.to_csv('train_aws_5000.csv',index=False)\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f596c2-7f71-48d3-8049-6c8fde2e6752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
