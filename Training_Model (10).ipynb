{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4709ec-4da4-41d9-9ec4-7ab13218de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5035)\n",
      "anomaly\n",
      " 1    981\n",
      "-1     19\n",
      "Name: count, dtype: int64\n",
      "Train labels:\n",
      " is_malicious\n",
      "0    2048\n",
      "1    1952\n",
      "Name: count, dtype: int64\n",
      "Test labels:\n",
      " is_malicious\n",
      "0    512\n",
      "1    488\n",
      "Name: count, dtype: int64\n",
      "Baseline evaluation\n",
      "[[469  43]\n",
      " [337 151]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.58      0.92      0.71       512\n",
      "     anomaly       0.78      0.31      0.44       488\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.68      0.61      0.58      1000\n",
      "weighted avg       0.68      0.62      0.58      1000\n",
      "\n",
      "Accuracy Score: 0.62\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Tuned model\n",
      "Best Parameters: {'contamination': 0.01, 'max_samples': 0.6, 'n_estimators': 100, 'random_state': 42}\n",
      "Best Score: 0.0\n",
      "Test Confusion:  [[512   0]\n",
      " [488   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.51      1.00      0.68       512\n",
      "     anomaly       0.00      0.00      0.00       488\n",
      "\n",
      "    accuracy                           0.51      1000\n",
      "   macro avg       0.26      0.50      0.34      1000\n",
      "weighted avg       0.26      0.51      0.35      1000\n",
      "\n",
      "Test accuracy: 0.512\n",
      "      severity_score severity_tier  anomaly\n",
      "4713       -0.001076      critical        0\n",
      "2697       -0.001636      critical        0\n",
      "3734       -0.001716      critical        0\n",
      "3307       -0.001543      critical        0\n",
      "1547       -0.001740      critical        0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "#We can import(numpy to handle fast computations for arrays)\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "#Imprt sklearn(open-source library for machine learning) in order to use Isolation Forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#We can import skllearn again in order to train/test our split variables, x(independent) and y(dependent)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Step 1: We must load the data from our CSV file using the with open block in order for us to read it\n",
    "df = pd.read_csv(\"train_aws_5000.csv\")\n",
    "\n",
    "#We get (10, 50) as our tuple, so this means we have 10 record and 50 features\n",
    "\n",
    "#We have to have at least 5000+ event objects in order for our datat anaylsis to be accurate\n",
    "print(df.shape)\n",
    "\n",
    "#Step 2: We must select features and target(is_malicious) and prepare data that is going to be used in our model\n",
    "#We must seperate our label columns into and split them into  x(features and y(target) variables\n",
    "#We use df.drop() in order to know what to drop and where to drop it\n",
    "\n",
    "#For axis, 0 means rows, and 1 means we are dropping the columns\n",
    "X = df.drop('is_malicious', axis=1)\n",
    "\n",
    "y = df['is_malicious']\n",
    "\n",
    "#We need to deal with any null values and fill in these values into 0\n",
    "X = X.fillna(0)\n",
    "\n",
    "#Lets use one time encoding to encode every value\n",
    "X = pd.get_dummies(X).astype(int)\n",
    "\n",
    "X = X.astype(float)\n",
    "\n",
    "#print(X.dtypes)\n",
    "\n",
    "\n",
    "\n",
    "#Step 3:We must train and test our model before we use our Isolation forest algorithmn to train and test out data\n",
    "\n",
    "#Method 1 - Manually split the data frame into test a dn test set\n",
    "\n",
    "#We can split our data into the train and test model - so the first 75% of rows will be in our training data set\n",
    "# and the rest will be in our test data set\n",
    "#sev_five_pct = 0.75 * df.shape[0]\n",
    "\n",
    "#The positions based filtering allows us to set the rows and the columns\n",
    "\n",
    "#train_set = df.iloc[:sev_five_pct-1, :]\n",
    "\n",
    "#test_set = df.iloc[sev_five_pct:, :]\n",
    "\n",
    "#Displays our training data(in rows and columns) and test_data(in rows and columns) in a tuple\n",
    "#train_set.shape, test_set.shape\n",
    "\n",
    "#Method 3 - Use the train_test_split present in the Sklearn in order to split your training data set and your test data set\n",
    "# You need more than 1 row of data in order for this method to work\n",
    "\n",
    "#straify allows us to tp preserves the original distribution of a certain variable(usally the taregt/label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "#BaseLine Isolation Forest\n",
    "\n",
    "#Step 4: We must use a Isolation Forest algorithmn(best used for anomaly detection)\n",
    "#n_estimators => Number of trees\n",
    "#Contination => Expected proportion of anomalies\n",
    "#Sample Size => number of samples used to train each tree\n",
    "isolation_forest_algo = IsolationForest(n_estimators=150, contamination=0.2, random_state=42)\n",
    "\n",
    "#You want to use only the training data in order to avoid leakage\n",
    "isolation_forest_algo.fit(X_train)\n",
    "\n",
    "#Step 5: Now we must calcualte the anomaly score for our scores and classify our anomalies\n",
    "#In our chase we are predicting the score on X_test\n",
    "X_test_data = df.loc[X_test.index].copy()\n",
    "\n",
    "#This helps us calcualte our anomaly score(higher score means more normal, lower means more anomalous)\n",
    "X_test_data['anomaly_score'] = isolation_forest_algo.decision_function(X_test)\n",
    "\n",
    "#We can classify anomalies with our threshold 0 (default in sklearn)\n",
    "X_test_data['anomaly'] = isolation_forest_algo.predict(X_test) # -1 for anomaly, 1 for normal\n",
    "\n",
    "#We access theand anomaly columns and it counts how \n",
    "#many times each unqiue value and teturns a series using the default threshold 0\n",
    "print(X_test_data['anomaly'].value_counts())\n",
    "\n",
    "#Step 6: We need a quick evaluation of our data through a classificaiton report and confusion matrix\n",
    "\n",
    "#Confusion Matrix - shows counts of true positives, true negatives, false positives and false negative\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#We need a y prediction value through the anomalies and we must convert our raw predictions into an integer\n",
    "#y_prediction = (X_test_data['anomaly'] == -1).astype(int)\n",
    "\n",
    "#Output: [[0 0] [2 0]], The confusion matrix TN(True Negative),FP(False Positive)\n",
    "#                                            FN(False Negative,TP(True Positive)\n",
    "\n",
    "#We are going to make our test size bigger so we dont have as many errors as we do here\n",
    "#Our events has 50 events(25 benign + 25 malicious)\n",
    "#We want to make sure we understand out matrix and the classification report\n",
    "\n",
    "#We are going to create our own threshold so that its not too high since our default value is 0\n",
    "\n",
    "# The decision function outputs the anomaly score for each sample\n",
    "# The lower the score, the anomalous the sample is\n",
    "scores = isolation_forest_algo.decision_function(X_test)\n",
    "\n",
    "#Our threshold is set at 20th scores(lowest 20% scores are considered anomalies)\n",
    "threshold = np.percentile(scores, 20)\n",
    "\n",
    "#We need a y prediction value through the anomalies and we must convert our raw predictions into an integer\n",
    "#This has created predicted labels(1 for anomaly(score below threshold), 0 for normal(score above threshold)\n",
    "y_pred = (scores < threshold).astype(int)\n",
    "\n",
    "# Shows how many normal(0) and anomaly(1) are in training and testing sets and checking if our data is balanced\n",
    "print(\"Train labels:\\n\", y_train.value_counts())\n",
    "print(\"Test labels:\\n\",  y_test.value_counts())\n",
    "\n",
    "# Now we are going to flag anomalies with our confusion matrix and classification report\n",
    "\n",
    "print(\"Baseline evaluation\")\n",
    "#Our Confusion Matrix:\n",
    "#True Negative(TN) - Correctly Classified as Normal\n",
    "#False Positive(FP) - Correctly Inccorrectly flagged as anomaly\n",
    "#False Negative(FN) - Anomaly event incorrectly flagge as normal\n",
    "#True Positive(TP) - Anomaly event correctlu flagged as anomaly\n",
    "print(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "\n",
    "\n",
    "#Classification:\n",
    "# Precision -> Of all predicted anomalies how many were actually anomalies? Equation Example: Precision for class 1 = TP/( TP + FP )\n",
    "# Recall -> Of all actual anomalies, how many did the model catch? Equation Example: Recall for class 1 = TP / (TP + FN)\n",
    "# F1-score: The mean of precision and recall(balance between the two)\n",
    "# Support: Numbers of true \n",
    "# 'zero_division=0' helps us avoid warnings if a class has zero predicted samples\n",
    "print(classification_report(y_test, y_pred,target_names=[\"normal\", \"anomaly\"],zero_division=0))\n",
    "\n",
    "#Now we are going to tune model accuracy(how close predictions made by machines with\n",
    "# the actual or true values. and reduce false positives\n",
    "\n",
    "#We can import out accuracy scores from sklearn to see just our accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy Score: {accuracy}')\n",
    "\n",
    "#We can tune the model accuracy by using hyper-parameter(external variables use to manage maching learning model training\n",
    "# grid search that searches for the best combination of hyperparameter values \n",
    "# We can use gridsearch CV or RandomizedSearchCV from sklearn\n",
    "\n",
    "#We are having having make scorer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Creates a function that maps IsoaltionForest output (-1/1) to 1/0, then F1\n",
    "# First, define the iso_f1_scorer function correctly\n",
    "def iso_f1_scorer(estimator,X_val, y_val):\n",
    "    # Convert isolation forest predictions (-1 for outliers, 1 for inliers)\n",
    "    # to binary format (1 for outliers, 0 for inliers)\n",
    "    y_pred_val = (estimator.predict(X_val) == -1).astype(int)\n",
    "    return f1_score(y_val, y_pred_val, zero_division=0)\n",
    "\n",
    "# The rest of your code remains the same\n",
    "hyper_parameter_grid = {'n_estimators': [100], \n",
    "                        'max_samples': [0.6,0.8, 1.0],\n",
    "                        'contamination': [0.01,0.02,0.30],\n",
    "                        'random_state': [42]\n",
    "                       }\n",
    "\n",
    "our_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(IsolationForest(random_state=42),\n",
    "                    param_grid=hyper_parameter_grid,\n",
    "                    scoring=iso_f1_scorer,\n",
    "                    cv=our_cv,\n",
    "                    n_jobs=1,\n",
    "                    verbose=1,\n",
    "                    error_score='raise')\n",
    "\n",
    "# Fit the grid search object into the training data that we have\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tuned model\")\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)\n",
    "\n",
    "best_if = grid.best_estimator_\n",
    "y_pred_best = (best_if.predict(X_test) == -1).astype(int)\n",
    "\n",
    "print(\"Test Confusion: \", confusion_matrix(y_test, y_pred_best, labels=[0,1]))\n",
    "print(classification_report(y_test, y_pred_best, target_names=[\"normal\", \"anomaly\"], zero_division=0))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "\n",
    "#Adding the scoring/severity logic to my code\n",
    "\n",
    "#Higher score = more normal -> invert\n",
    "X_test_data = df.loc[X_test.index].copy()\n",
    "X_test_data[\"anomaly_score\"]  = scores\n",
    "X_test_data[\"severity_score\"] = -scores\n",
    "X_test_data[\"anomaly\"] = (scores < threshold).astype(int)\n",
    "\n",
    "#bucket this into tiers\n",
    "bins = [-np.inf, 0.05, 0.15, 0.30, np.inf]\n",
    "\n",
    "#Our labels for our \n",
    "labels = ['critical', 'high', 'medium', 'low']\n",
    "\n",
    "X_test_data[\"severity_tier\"] = pd.cut(X_test_data[\"severity_score\"], bins=bins, labels=labels) \n",
    "\n",
    "print(X_test_data[[\"severity_score\", \"severity_tier\", \"anomaly\"]].head())\n",
    "\n",
    "#Now we are saving our model for deployment\n",
    "import joblib\n",
    "\n",
    "#Save the best model for deployment\n",
    "joblib.dump(best_if, \"first_pipeline.pkl\")\n",
    "\n",
    "model = joblib.load(\"first_pipeline.pkl\")\n",
    "thresh = 0.05\n",
    "\n",
    "def handler(event, ctx):\n",
    "    df = pd.DataFrame(event[\"records\"])\n",
    "    scores = model.decision_function(df)\n",
    "    preds = (scores < thresh).astype(int)\n",
    "    return {\"predictions\": preds.tolist(), \"scores\": scores.tolist()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481a854-cd7c-4de3-9f04-00a757d54402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917fbb1-da7e-470b-9111-a2dafa4045b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
